import openai
import anthropic
import aiohttp
from dotenv import load_dotenv
import os
import re
import subprocess
import sys
import tempfile
import copy
import json
import asyncio
import argparse
from itertools import zip_longest
from utils import (
    is_number_string,
    convert_to_number,
    extract_best_objective,
    eval_model_result
)

# Load environment variables from .env file
load_dotenv()

# OpenAI API setup
openai_api_data = dict(
    api_key = os.getenv("OPENAI_API_KEY"),
    base_url = os.getenv("OPENAI_API_BASE")
)

# Anthropic API setup
anthropic_api_data = dict(
    api_key = os.getenv("CLAUDE_API_KEY"),
)

# Ollama API setup
ollama_api_data = dict(
    base_url = os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
)

# Initialize clients
openai_client = openai.AsyncOpenAI(
    api_key=openai_api_data['api_key'],
    base_url=openai_api_data['base_url'] if openai_api_data['base_url'] else None
)

anthropic_client = anthropic.AsyncAnthropic(
    api_key=anthropic_api_data['api_key']
)

# No client initialization needed for Ollama, we'll use aiohttp directly

async def async_query_llm(messages, model_name="o3-mini", temperature=0.2):
    """
    Async version of query_llm that supports OpenAI, Claude, and Ollama models
    
    Args:
        messages (list): List of conversation context.
        model_name (str): LLM model name, default is "o3-mini".
                         For Ollama models, prefix with "ollama:" (e.g., "ollama:llama2")
        temperature (float): Controls the randomness of output, default is 0.2.

    Returns:
        str: Response content generated by the LLM.
    """
    # Check if model is Ollama
    if model_name.lower().startswith("ollama:"):
        # Extract the actual model name after the "ollama:" prefix
        ollama_model = model_name.split(":", 1)[1]
        
        # Prepare the request payload
        payload = {
            "model": ollama_model,
            "messages": messages,
            "temperature": temperature,
            "stream": False
        }
        
        # Make the API request to Ollama using aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{ollama_api_data['base_url']}/api/chat",
                json=payload
            ) as response:
                # Check if the request was successful
                if response.status == 200:
                    result = await response.json()
                    return result["message"]["content"]
                else:
                    error_text = await response.text()
                    error_msg = f"Ollama API error: {response.status} - {error_text}"
                    print(error_msg)
                    return f"Error: {error_msg}"
    
    # Check if model is Claude (Anthropic)
    elif model_name.lower().startswith("claude"):
        # Convert OpenAI message format to Anthropic format
        system_message = next((m["content"] for m in messages if m["role"] == "system"), "")
        user_messages = [m["content"] for m in messages if m["role"] == "user"]
        assistant_messages = [m["content"] for m in messages if m["role"] == "assistant"]
        
        # Combine messages into a single conversation string
        conversation = system_message + "\n\n"
        for user_msg, asst_msg in zip_longest(user_messages, assistant_messages, fillvalue=None):
            if user_msg:
                conversation += f"Human: {user_msg}\n\n"
            if asst_msg:
                conversation += f"Assistant: {asst_msg}\n\n"
        
        # Add the final user message if there is one
        if len(user_messages) > len(assistant_messages):
            conversation += f"Human: {user_messages[-1]}\n\n"

        response = await anthropic_client.messages.create(
            model=model_name,
            max_tokens=8192,
            temperature=temperature,
            messages=[{
                "role": "user",
                "content": conversation
            }]
        )
        return response.content[0].text
    else:
        # Use OpenAI API
        response = await openai_client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature
        )
        return response.choices[0].message.content

async def async_extract_and_execute_python_code(text_content):
    """
    Async version of extract_and_execute_python_code
    """
    python_code_blocks = re.findall(r'```python\s*([\s\S]*?)```', text_content)

    if not python_code_blocks:
        print("No Python code blocks found.")
        return False, "No Python code blocks found"

    for code_block in python_code_blocks:
        code_block = code_block.strip()
        if not code_block:
            print("Found an empty Python code block, skipped.")
            continue

        print("Found Python code block, starting execution...")
        try:
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as tmp_file:
                tmp_file.write(code_block)
                temp_file_path = tmp_file.name

            proc = await asyncio.create_subprocess_exec(
                sys.executable,
                temp_file_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await proc.communicate()
            
            if proc.returncode == 0:
                print("Python code executed successfully, output:\n")
                stdout_str = stdout.decode()
                print(stdout_str)
                
                best_obj = extract_best_objective(stdout_str)
                if best_obj is not None:
                    print(f"\nBest objective value: {best_obj}")
                else:
                    print("\nNo optimal solution value found")
                return True, str(best_obj)
            else:
                print(f"Python code execution error, error message:\n")
                print(stderr.decode())
                return False, stderr.decode()

        except Exception as e:
            print(f"Error occurred while executing Python code block: {e}")
            return False, str(e)
        finally:
            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                os.remove(temp_file_path)
        print("-" * 30)

    return False, "No valid code blocks executed"

async def async_gpt_code_agent_simple(user_question, model_name="o3-mini", max_attempts=3):
    """
    Async version of gpt_code_agent_simple
    """
    messages = [
        {"role": "system", "content": (
            "You are an operations research expert. Based on the optimization problem provided by the user, construct a mathematical model and write complete, reliable Python code using Gurobi to solve the operations research optimization problem."
            "The code should include necessary model construction, variable definitions, constraint additions, objective function settings, as well as solving and result output."
            "Output in the format ```python\n{code}\n```, without code explanations."
        )},
        {"role": "user", "content": user_question}
    ]

    gurobi_code = await async_query_llm(messages, model_name)
    print("[Python Gurobi Code]:\n", gurobi_code)
    text = f"{gurobi_code}"
    is_solve_success, result = await async_extract_and_execute_python_code(text)
    
    print(f'Stage result: {is_solve_success}, {result}')
    
    return is_solve_success, result

async def async_generate_or_code_solver(messages_bak, model_name, max_attempts):
    messages = copy.deepcopy(messages_bak)

    gurobi_code = await async_query_llm(messages, model_name)
    print("[Python Gurobi Code]:\n", gurobi_code)

    text = f"{gurobi_code}"
    attempt = 0
    while attempt < max_attempts:
        success, error_msg = await async_extract_and_execute_python_code(text)
        if success:
            messages_bak.append({"role": "assistant", "content": gurobi_code})
            return True, error_msg, messages_bak

        print(f"\nAttempt {attempt + 1} failed, requesting LLM to fix code...\n")

        messages.append({"role": "assistant", "content": gurobi_code})
        messages.append({"role": "user", "content": f"Code execution encountered an error, error message is as follows:\n{error_msg}\nPlease fix the code and provide the complete executable code again."})

        gurobi_code = await async_query_llm(messages, model_name)
        text = f"{gurobi_code}"

        print("\nReceived fixed code, preparing to execute again...\n")
        attempt += 1

    messages_bak.append({"role": "assistant", "content": gurobi_code})
    print(f"Reached maximum number of attempts ({max_attempts}), could not execute code successfully.")
    return False, None, messages_bak

async def async_or_llm_agent(user_question, model_name="o3-mini", max_attempts=3):
    """
    Async version of or_llm_agent function.
    """
    # Initialize conversation history
    messages = [
        {"role": "system", "content": (
            "You are an operations research expert. Based on the optimization problem provided by the user, construct a mathematical model that effectively models the original problem using mathematical (linear programming) expressions.\n\n"
            "Follow these steps:\n"
            "1. Identify the decision variables and clearly define what each variable represents\n"
            "2. Formulate the objective function (min or max)\n"
            "3. List all constraints with clear mathematical expressions\n"
            "4. Specify any bounds or restrictions on variables\n\n"
            "Focus on obtaining a correct and complete mathematical model. This model will be used later to guide the generation of Gurobi code."
        )},
        {"role": "user", "content": user_question}
    ]

    # 1. Generate mathematical model
    math_model = await async_query_llm(messages, model_name)
    print("[Mathematical Model]:\n", math_model)
    
    validate_math_model = math_model
    messages.append({"role": "assistant", "content": validate_math_model})
    
    messages.append({"role": "user", "content": (
        "Based on the above mathematical model, write complete and reliable Python code using Gurobi to solve this operations research optimization problem.\n\n"
        "Your code must follow this structure:\n"
        "1. Import necessary libraries (gurobipy, numpy, etc.)\n"
        "2. Create a model instance\n"
        "3. Define and add variables with appropriate bounds\n"
        "4. Set the objective function\n"
        "5. Add all constraints\n"
        "6. Optimize the model\n"
        "7. Extract and print the results, including the optimal objective value\n"
        "8. Handle potential infeasibility or unboundedness\n\n"
        "Output in the format ```python\n{code}\n```, without code explanations."
    )})

    # copy msg; solve; add the last gurobi code 
    is_solve_success, result, messages = await async_generate_or_code_solver(messages, model_name, max_attempts)
    print(f'Stage result: {is_solve_success}, {result}')
    
    if is_solve_success:
        if not is_number_string(result):
            print('!![No available solution warning]!!')
            messages.append({"role": "user", "content": (
                "The current model resulted in *no feasible solution*. This indicates one of these issues:\n"
                "1. Contradictory constraints making the problem infeasible\n"
                "2. Incorrect variable bounds\n"
                "3. Errors in constraint formulation\n\n"
                "Please carefully analyze the mathematical model and Gurobi code. Add diagnostic code to identify which constraints are causing infeasibility. Then fix the issues and provide the complete corrected code.\n\n"
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
            is_solve_success, result, messages = await async_generate_or_code_solver(messages, model_name, max_attempts=1)
    else:
        print('!![Max attempt debug error warning]!!')
        messages.append({"role": "user", "content": (
                "The model code still reports errors after multiple debugging attempts. Here are common issues to address:\n"
                "1. Check for syntax errors or undefined variables\n"
                "2. Ensure all constraints use proper Gurobi syntax (e.g., model.addConstr() not just expressions)\n"
                "3. Verify that all mathematical operations are valid (e.g., no division by zero)\n"
                "4. Confirm that variable types match their usage (continuous vs. integer vs. binary)\n\n"
                "Please completely rebuild the Gurobi Python code with careful attention to these details.\n"
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
        is_solve_success, result, messages = await async_generate_or_code_solver(messages, model_name, max_attempts=2)
    
    return is_solve_success, result

async def process_single_case(i, d, args):
    """
    Process a single test case
    """
    print(f"=============== num {i} ==================")
    user_question, answer = d['question'], d['answer']
    print(user_question)
    print('-------------')
    
    if args.agent:
        is_solve_success, llm_result = await async_or_llm_agent(user_question, args.model)
    else:
        is_solve_success, llm_result = await async_gpt_code_agent_simple(user_question, args.model)
        
    if is_solve_success:
        print(f"Successfully executed code, optimal solution value: {llm_result}")
    else:
        print("Failed to execute code.")
    print('------------------')
    
    pass_flag, correct_flag = eval_model_result(is_solve_success, llm_result, answer)
    
    print(f'solve: {is_solve_success}, llm: {llm_result}, ground truth: {answer}')
    print(f'[Final] run pass: {pass_flag}, solve correct: {correct_flag}')
    print(' ')
    
    return llm_result, pass_flag, correct_flag, i

def parse_args():
    """
    Parse command line arguments.
    
    Returns:
        argparse.Namespace: The parsed arguments
    """
    parser = argparse.ArgumentParser(description='Run optimization problem solving with LLMs (async version)')
    parser.add_argument('--agent', action='store_true', 
                        help='Use the agent. If not specified, directly use the model to solve the problem')
    parser.add_argument('--model', type=str, default='o3-mini',
                        help='Model name to use for LLM queries. Use "claude-..." for Claude models or "ollama:..." for Ollama models.')
    parser.add_argument('--data_path', type=str, default='data/datasets/dataset_combined_result.json',
                        help='Path to the dataset JSON file')
    return parser.parse_args()

async def main():
    # Load dataset from JSON file
    args = parse_args()
    
    with open(args.data_path, 'r') as f:
        dataset = json.load(f)
    
    # Create tasks for all test cases
    tasks = []
    for i, d in dataset.items():
        task = process_single_case(i, d, args)
        tasks.append(task)
    
    # Run all tasks concurrently and gather results
    results = await asyncio.gather(*tasks)
    
    # Process results
    pass_count = sum(1 for _, pass_flag, _, _ in results if pass_flag)
    correct_count = sum(1 for _, _, correct_flag, _ in results if correct_flag)
    error_datas = [i for _, pass_flag, correct_flag, i in results if not pass_flag or not correct_flag]
    
    print(f'[Total {len(dataset)}] run pass: {pass_count}, solve correct: {correct_count}')
    print(f'[Total fails {len(error_datas)}] error datas: {error_datas}')

if __name__ == "__main__":
    # Running as script
    asyncio.run(main())
