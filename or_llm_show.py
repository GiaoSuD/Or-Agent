import openai
import requests
from dotenv import load_dotenv
import os
import re
import subprocess
import sys
import tempfile
import copy
import json
import shutil
import wcwidth
import json
from rich.console import Console
from rich.markdown import Markdown
import io 
from contextlib import redirect_stdout
import time
from utils import (
    is_number_string,
    convert_to_number,
    extract_best_objective,
    extract_and_execute_python_code,
    eval_model_result
)
# Load environment variables from .env file
load_dotenv()

# Load environment variables from .env file
load_dotenv()

# OpenAI API setup
openai_api_data = dict(
    api_key = os.getenv("OPENAI_API_KEY"),
    base_url = os.getenv("OPENAI_API_BASE")
)

# Ollama API setup
ollama_api_data = dict(
    base_url = os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
)

# Initialize OpenAI client
client = openai.OpenAI(
    api_key=openai_api_data['api_key'],
    base_url=openai_api_data['base_url'] if openai_api_data['base_url'] else None
)

# No client initialization needed for Ollama, we'll use requests directly

def get_display_width(text):
    """
    Calculate the display width of a string, accounting for wide characters like Chinese.
    Uses the wcwidth module for accurate width calculation.
    
    Args:
        text (str): The text to calculate the width for.
        
    Returns:
        int: The display width of the text.
    """
    return wcwidth.wcswidth(text)


def print_header(text="", add_newline_before=True, add_newline_after=True, 
                border_char="=", side_char="||"):
    """
    Print a header with customizable text in the middle, adjusted to the console window width.
    Properly handles wide characters like Chinese.
    
    Args:
        text (str): The text to display in the middle of the header.
        add_newline_before (bool): Whether to add a newline before the header.
        add_newline_after (bool): Whether to add a newline after the header.
        border_char (str): Character to use for the top and bottom borders.
        side_char (str): Character to use for the side borders.
    """
    # Add a newline before the header if requested
    if add_newline_before:
        print()
    
    # Get terminal width
    # try:
    terminal_width = shutil.get_terminal_size().columns
    # except Exception:
    #     # Fallback width if terminal size cannot be determined
    #     terminal_width = 80
    
    # Ensure minimum width
    terminal_width = max(terminal_width, 40)
    
    # Calculate side character padding
    side_char_len = len(side_char)
    
    # Print the top border
    print(border_char * terminal_width)
    
    # Print the empty line
    print(side_char + " " * (terminal_width - 2 * side_char_len) + side_char)
    
    # Print the middle line with text
    text_display_width = get_display_width(text)
    available_space = terminal_width - 2 * side_char_len
    
    if text_display_width <= available_space:
        left_padding = (available_space - text_display_width) // 2
        right_padding = available_space - text_display_width - left_padding
        # print(terminal_width, text_display_width, available_space, left_padding, right_padding)
        print(side_char + " " * left_padding + text + " " * right_padding + side_char)
    else:
        # If text is too long, we need to truncate it
        # This is more complex with wide characters, so we'll do it character by character
        truncated_text = ""
        truncated_width = 0
        for char in text:
            char_width = get_display_width(char)
            if truncated_width + char_width + 3 > available_space:  # +3 for the "..."
                break
            truncated_text += char
            truncated_width += char_width
        
        truncated_text += "..."
        right_padding = available_space - get_display_width(truncated_text)
        print(side_char + truncated_text + " " * right_padding + side_char)
    
    # Print the empty line
    print(side_char + " " * (terminal_width - 2 * side_char_len) + side_char)
    
    # Print the bottom border
    print(border_char * terminal_width)
    
    # Add a newline after the header if requested
    if add_newline_after:
        print()

def query_llm(messages, model_name="o3-mini", temperature=0.2):
    """
    Call LLM to get response results using streaming output.
    
    Args:
        messages (list): List of conversation context.
        model_name (str): LLM model name, default is "gpt-4".
                         For Ollama models, prefix with "ollama:" (e.g., "ollama:llama2")
        temperature (float): Controls the randomness of output, default is 0.2.

    Returns:
        str: Response content generated by the LLM.
    """
    # For accumulating the complete response
    full_response = ""
    
    # For controlling the print format
    print("LLM Output: ", end="", flush=True)
    
    # Check if model is Ollama
    if model_name.lower().startswith("ollama:"):
        # Extract the actual model name after the "ollama:" prefix
        ollama_model = model_name.split(":", 1)[1]
        
        # Prepare the request payload
        payload = {
            "model": ollama_model,
            "messages": messages,
            "temperature": temperature,
            "stream": True
        }
        
        # Make the API request to Ollama with streaming
        response = requests.post(
            f"{ollama_api_data['base_url']}/api/chat",
            json=payload,
            stream=True
        )
        
        # Check if the request was successful
        if response.status_code == 200:
            # Process the streaming response
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line)
                        if "message" in chunk and "content" in chunk["message"]:
                            content = chunk["message"]["content"]
                            if content:
                                print(content, end="", flush=True)
                                full_response += content
                    except json.JSONDecodeError:
                        pass
        else:
            error_msg = f"Ollama API error: {response.status_code} - {response.text}"
            print(error_msg)
            full_response = f"Error: {error_msg}"
    else:
        # Use OpenAI API with streaming
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
            stream=True
        )
        
        # Process streaming response chunk by chunk
        for chunk in response:
            # First check if the choices list is not empty
            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                # Then check if there is delta and content
                if hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content'):
                    content = chunk.choices[0].delta.content
                    if content:
                        print(content, end="", flush=True)
                        full_response += content
    
    # Line break after output is complete
    print()
    
    return full_response

def generate_or_code_solver(messages_bak, model_name, max_attempts):
    messages = copy.deepcopy(messages_bak)
    
    print_header("LLM Generating Python Gurobi Code")

    gurobi_code = query_llm(messages, model_name)

    print_header("Automatically Executing Python Code")
    # 4. Code execution & fixes
    text = f"{gurobi_code}"
    attempt = 0
    while attempt < max_attempts:
        buffer2 = io.StringIO()
        with redirect_stdout(buffer2):
            success, error_msg = extract_and_execute_python_code(text)
        captured_output2 = buffer2.getvalue()
        for c in captured_output2:
            print(c, end="", flush=True)
            time.sleep(0.005)

        if success:
            messages_bak.append({"role": "assistant", "content": gurobi_code})
            return True, error_msg, messages_bak

        print(f"\nAttempt {attempt + 1} failed, requesting LLM to fix code...\n")

        # Build repair request
        messages.append({"role": "assistant", "content": gurobi_code})
        messages.append({"role": "user", "content": f"Code execution encountered an error, error message is as follows:\n{error_msg}\nPlease fix the code and provide the complete executable code again."})

        # Get the fixed code
        gurobi_code = query_llm(messages, model_name)
        text = f"{gurobi_code}"

        print("\nReceived fixed code, preparing to execute again...\n")
        attempt += 1
    # not add gurobi code
    messages_bak.append({"role": "assistant", "content": gurobi_code})
    print(f"Reached maximum number of attempts ({max_attempts}), could not execute code successfully.")
    return False, None, messages_bak


def or_llm_agent(user_question, model_name="o3-mini", max_attempts=3):
    """
    Request Gurobi code solution from LLM and execute it, attempt to fix if it fails.

    Args:
        user_question (str): User's problem description.
        model_name (str): LLM model name to use, default is "gpt-4".
        max_attempts (int): Maximum number of attempts, default is 3.

    Returns:
        tuple: (success: bool, best_objective: float or None, final_code: str)
    """
    # Initialize conversation history
    messages = [
        {"role": "system", "content": (
            "You are an operations research expert. Based on the optimization problem provided by the user, construct a mathematical model that effectively models the original problem using mathematical (linear programming) expressions.\n\n"
            "Follow these steps:\n"
            "1. Identify the decision variables and clearly define what each variable represents\n"
            "2. Formulate the objective function (min or max)\n"
            "3. List all constraints with clear mathematical expressions\n"
            "4. Specify any bounds or restrictions on variables\n\n"
            "Focus on obtaining a correct and complete mathematical model. This model will be used later to guide the generation of Gurobi code."
        )},
        {"role": "user", "content": user_question}
    ]

    # 1. Generate mathematical model
    print_header("LLM Reasoning to Build Linear Programming Model")
    math_model = query_llm(messages, model_name)
    # print("【数学模型】:\n", math_model)

    # # 2. Validate mathematical model
    # messages.append({"role": "assistant", "content": math_model})
    # messages.append({"role": "user", "content": (
    #     "Please check if the above mathematical model matches the problem description. If there are errors, make corrections; if there are no errors, check if it can be optimized."
    #     "In any case, please output the final mathematical model again."
    # )})

    # validate_math_model = query_llm(messages, model_name)
    # print("[Validated Mathematical Model]:\n", validate_math_model)
    
    validate_math_model = math_model
    messages.append({"role": "assistant", "content": validate_math_model})
    
    # ------------------------------
    messages.append({"role": "user", "content": (
        "Based on the above mathematical model, write complete and reliable Python code using Gurobi to solve this operations research optimization problem.\n\n"
        "Your code must follow this structure:\n"
        "1. Import necessary libraries (gurobipy, numpy, etc.)\n"
        "2. Create a model instance\n"
        "3. Define and add variables with appropriate bounds\n"
        "4. Set the objective function\n"
        "5. Add all constraints\n"
        "6. Optimize the model\n"
        "7. Extract and print the results, including the optimal objective value\n"
        "8. Handle potential infeasibility or unboundedness\n\n"
        "Output in the format ```python\n{code}\n```, without code explanations."
    )})
    # copy msg; solve; add the laset gurobi code 
    is_solve_success, result, messages = generate_or_code_solver(messages, model_name,max_attempts)
    print(f'Stage result: {is_solve_success}, {result}')
    if is_solve_success:
        if not is_number_string(result):
            print('!![No available solution warning]!!')
            # no solution 
            messages.append({"role": "user", "content": (
                "The current model resulted in *no feasible solution*. This indicates one of these issues:\n"
                "1. Contradictory constraints making the problem infeasible\n"
                "2. Incorrect variable bounds\n"
                "3. Errors in constraint formulation\n\n"
                "Please carefully analyze the mathematical model and Gurobi code. Add diagnostic code to identify which constraints are causing infeasibility. Then fix the issues and provide the complete corrected code.\n\n"
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
            is_solve_success, result, messages = generate_or_code_solver(messages, model_name, max_attempts=1)
    else:
        print('!![Max attempt debug error warning]!!')
        messages.append({"role": "user", "content": (
                "The model code still reports errors after multiple debugging attempts. Here are common issues to address:\n"
                "1. Check for syntax errors or undefined variables\n"
                "2. Ensure all constraints use proper Gurobi syntax (e.g., model.addConstr() not just expressions)\n"
                "3. Verify that all mathematical operations are valid (e.g., no division by zero)\n"
                "4. Confirm that variable types match their usage (continuous vs. integer vs. binary)\n\n"
                "Please completely rebuild the Gurobi Python code with careful attention to these details.\n"
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
        is_solve_success, result, messages = generate_or_code_solver(messages, model_name, max_attempts=2)
    
    return is_solve_success, result



if __name__ == "__main__":
    with open('data/datasets/dataset_md_result.json', 'r') as f:
        dataset = json.load(f)
    # print(dataset['0'])
    console = Console()

    model_name = 'o3-mini'
    # model_name = ''

    # model_name = 'Pro/deepseek-ai/DeepSeek-R1' 
    # model_name = 'deepseek-reasoner'

    pass_count = 0
    correct_count = 0
    for i, d in dataset.items():
        #print(i)
        # if int(i) in [0]:
        print_header("Operations Research Optimization Problem")
        user_question, answer = d['question'], d['answer']
        # print(user_question)
        buffer2 = io.StringIO()
        with redirect_stdout(buffer2):
            md = Markdown(user_question)
            console.print(md)
            print('-------------')

        captured_output2 = buffer2.getvalue()
        for c in captured_output2:
            print(c, end="", flush=True)
            time.sleep(0.005)
        is_solve_success, llm_result = or_llm_agent(user_question, model_name)
        # is_solve_success, llm_result = gpt_code_agent_simple(user_question, model_name)
        if is_solve_success:
            print(f"Successfully executed code, optimal solution value: {llm_result}")
        else:
            print("Failed to execute code.")
